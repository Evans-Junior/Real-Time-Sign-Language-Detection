{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import Libraries\n",
    "\n",
    "Notr I did this part of the work on google colab so you might have to install the libraries which I have not added to the requirements.txt file by yourself, thank you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tW_-hRecyFg"
   },
   "outputs": [],
   "source": [
    "# Define the directory where all the pickle files are stored\n",
    "pickle_dir = './pickle_data'  # Update with the path to your pickle files\n",
    "\n",
    "# Initialize lists to hold data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop through all the pickle files in the specified directory\n",
    "for filename in os.listdir(pickle_dir):\n",
    "    if filename.endswith('.pickle'):\n",
    "        with open(os.path.join(pickle_dir, filename), 'rb') as f:\n",
    "            data_dict = pickle.load(f)\n",
    "\n",
    "            # Add data and labels from the current pickle file\n",
    "            data.extend(data_dict['data'])\n",
    "            labels.extend(data_dict['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "data = np.asarray(data)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiHeUfl4djO3",
    "outputId": "96a62e80-ae6f-4ea3-8ea5-ca60284c2aa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.05546527, 0.56381139, 0.15827014, ..., 0.07150623, 0.03433628,\n",
       "         0.12176374],\n",
       "        [0.05652921, 0.53700131, 0.15578946, ..., 0.06563127, 0.05089188,\n",
       "         0.12211096],\n",
       "        [0.0599577 , 0.52526981, 0.15734272, ..., 0.0648033 , 0.04862627,\n",
       "         0.11821669],\n",
       "        ...,\n",
       "        [0.        , 0.33251974, 0.03970164, ..., 0.43121585, 0.15227744,\n",
       "         0.40552565],\n",
       "        [0.        , 0.33551592, 0.03773062, ..., 0.43510818, 0.14947781,\n",
       "         0.41032994],\n",
       "        [0.        , 0.33078617, 0.03748596, ..., 0.43224794, 0.15270759,\n",
       "         0.40866202]]),\n",
       " array(['4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4',\n",
       "        '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4',\n",
       "        '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4',\n",
       "        '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4',\n",
       "        '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4',\n",
       "        '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4',\n",
       "        '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4',\n",
       "        '4', '4', '4', '4', '4', '4', '4', '4', '4', '5', '5', '5', '5',\n",
       "        '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5',\n",
       "        '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5',\n",
       "        '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5',\n",
       "        '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5',\n",
       "        '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5',\n",
       "        '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5',\n",
       "        '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5',\n",
       "        '5', '5', '5', '5', '5', '7', '7', '7', '7', '7', '7', '7', '7',\n",
       "        '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7',\n",
       "        '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7',\n",
       "        '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7',\n",
       "        '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7',\n",
       "        '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7',\n",
       "        '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7',\n",
       "        '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7', '7',\n",
       "        '7', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0',\n",
       "        '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '8', '8', '8',\n",
       "        '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8',\n",
       "        '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8',\n",
       "        '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8',\n",
       "        '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8',\n",
       "        '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8',\n",
       "        '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8',\n",
       "        '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8', '8',\n",
       "        '8', '8', '8', '8', '8', '8', '3', '3', '3', '3', '3', '3', '3',\n",
       "        '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3',\n",
       "        '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3',\n",
       "        '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3',\n",
       "        '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3',\n",
       "        '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3',\n",
       "        '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3',\n",
       "        '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3',\n",
       "        '3', '3', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2',\n",
       "        '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '9', '9',\n",
       "        '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9',\n",
       "        '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9',\n",
       "        '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9',\n",
       "        '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9',\n",
       "        '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9',\n",
       "        '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9',\n",
       "        '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9', '9',\n",
       "        '9', '9', '9', '9', '9', '9', '9', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1',\n",
       "        '1', '1', '1', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6',\n",
       "        '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6',\n",
       "        '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6',\n",
       "        '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6',\n",
       "        '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6',\n",
       "        '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6',\n",
       "        '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6',\n",
       "        '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6', '6'],\n",
       "       dtype='<U1'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Feature Engineering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels as integers (if not already in numeric format)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Split Data For Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets using stratified sampling\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, y_encoded, test_size=0.2, shuffle=True, stratify=y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Train the model Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predict = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evaluation of my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryTnB-CTdiWh",
    "outputId": "f24228eb-7c32-4b6c-bc2e-b4fa22736365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00% of samples were classified correctly!\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "score = accuracy_score(y_predict, y_test)\n",
    "print(f'{score * 100:.2f}% of samples were classified correctly!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_lwdNgRCd_hp",
    "outputId": "47e42bed-4930-462a-a649-80529c304887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a pickle file\n",
    "with open('model.p', 'wb') as f:\n",
    "    pickle.dump({'model': model}, f)\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Making analysis of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1JBP1_heCeg",
    "outputId": "98f305d0-16dc-493d-d257-6f64c30947ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[20  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 20  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 20  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 20  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 20  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 20  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 20  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 20  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        20\n",
      "           1       1.00      1.00      1.00        20\n",
      "           2       1.00      1.00      1.00        20\n",
      "           3       1.00      1.00      1.00        20\n",
      "           4       1.00      1.00      1.00        20\n",
      "           5       1.00      1.00      1.00        20\n",
      "           6       1.00      1.00      1.00        20\n",
      "           7       1.00      1.00      1.00        20\n",
      "           8       1.00      1.00      1.00        20\n",
      "           9       1.00      1.00      1.00        20\n",
      "\n",
      "    accuracy                           1.00       200\n",
      "   macro avg       1.00      1.00      1.00       200\n",
      "weighted avg       1.00      1.00      1.00       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_predict)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Display classification report\n",
    "class_report = classification_report(y_test, y_predict)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#COMMENTS\n",
    "The model is perfect, it was able to reach all the standards required to be able to trust the results from a prediction.\n",
    "\n",
    "The confusion matrix also says the same story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Backing up data on the Google Clound, since I used Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!Note please don't run this part if you are not using google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCzjR5seePZx",
    "outputId": "b31215fc-0f81-454b-cd0e-24155cd3fbe6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YUAB_YvEfm6t"
   },
   "outputs": [],
   "source": [
    "pickle_dir = '/content/drive/MyDrive/dataset/cloudcomputing/models/'  # Update with the path to your Google Drive folder\n",
    "os.makedirs(pickle_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTtAilkAfr_C",
    "outputId": "e8bceca9-0005-4b3c-f77b-9967d6cdaf5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at /content/drive/MyDrive/dataset/cloudcomputing/models/random_forest_model.pickle\n"
     ]
    }
   ],
   "source": [
    "model_file_path = os.path.join(pickle_dir, 'random_forest_model.pickle')\n",
    "\n",
    "\n",
    "with open(model_file_path, 'wb') as f:\n",
    "    pickle.dump({'model': model}, f)\n",
    "\n",
    "print(f\"Model saved successfully at {model_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
